{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6SDFupGoh4qiclvGncKKi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohki-yu0225/social_media_analysis/blob/main/text_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ソーシャルメディア分析・入門(3)：テキスト分析演習\n",
        "\n",
        "【内容】\n",
        "- データの収集\n",
        "- 形態素解析\n",
        "- 頻度分析\n",
        "- ベクトル表現"
      ],
      "metadata": {
        "id": "tE4FnhhzrnSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ライブラリのインポート\n",
        "\n",
        "テキスト分析やYoutubeデータの収集に必要なライブラリをインポートする。"
      ],
      "metadata": {
        "id": "-1Y2PJBzuIqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize_matplotlib\n",
        "!pip install janome\n",
        "!pip install sentence-transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import japanize_matplotlib\n",
        "import googleapiclient.discovery\n",
        "from googleapiclient.discovery import build\n",
        "from janome.tokenizer import Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "import re\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "q35QIM0qupO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## データの収集\n",
        "\n",
        "### Youtube APIによるコメントデータの収集\n",
        "\n",
        "Youtube Data APIを用いて，動画に対するコメントのデータを取得する。Youtubeでは，動画ごとにIDが割り振られており，IDを指定することでAPIを通じてデータを取得する。\n",
        "\n",
        "【参考】[Youtube Data API](https://developers.google.com/youtube/v3?hl=ja)\n",
        "\n",
        "\n",
        "【参考】[Youtube Data APIで動画の情報を収集](https://qiita.com/nbayashi/items/bde26cd04f08de21d552)"
      ],
      "metadata": {
        "id": "7eGPip1XAN9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comment(api_key, video_id):\n",
        "    # Disable OAuthlib\"s HTTPS verification when running locally.\n",
        "    # *DO NOT* leave this option enabled in production.\n",
        "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "    api_service_name = \"youtube\"\n",
        "    api_version = \"v3\"\n",
        "    DEVELOPER_KEY = api_key\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
        "\n",
        "    # 動画情報を取得\n",
        "    video_request = youtube.videos().list(\n",
        "        part=\"snippet,statistics\",\n",
        "        id=video_id\n",
        "    )\n",
        "    video_response = video_request.execute()\n",
        "\n",
        "    # 動画情報を表示\n",
        "    if video_response[\"items\"]:\n",
        "        video_info = video_response[\"items\"][0][\"snippet\"]\n",
        "        video_stats = video_response[\"items\"][0][\"statistics\"]\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"【動画情報】\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"タイトル: {video_info[\"title\"]}\")\n",
        "        print(f\"公開者: {video_info[\"channelTitle\"]}\")\n",
        "        print(f\"公開日: {video_info[\"publishedAt\"]}\")\n",
        "        print(f\"再生回数: {video_stats.get(\"viewCount\", \"N/A\")}\")\n",
        "        print(f\"高評価数: {video_stats.get(\"likeCount\", \"N/A\")}\")\n",
        "        print(f\"コメント数: {video_stats.get(\"commentCount\", \"N/A\")}\")\n",
        "        print(\"=\" * 60)\n",
        "        print()\n",
        "\n",
        "    # コメントを取得\n",
        "    comments = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet,replies\",\n",
        "            videoId=video_id,\n",
        "            pageToken=page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response[\"items\"]:\n",
        "            # For top-level comments\n",
        "            comment_snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "            comments.append({\n",
        "                \"author\": comment_snippet[\"authorDisplayName\"],\n",
        "                \"comment\": comment_snippet[\"textDisplay\"],\n",
        "                \"date\": comment_snippet[\"publishedAt\"],\n",
        "                \"like_count\": comment_snippet[\"likeCount\"]\n",
        "            })\n",
        "            # For replies\n",
        "            if item[\"snippet\"][\"totalReplyCount\"] > 0:\n",
        "                for reply_item in item[\"replies\"][\"comments\"]:\n",
        "                    reply_snippet = reply_item[\"snippet\"]\n",
        "                    comments.append({\n",
        "                        \"author\": reply_snippet[\"authorDisplayName\"],\n",
        "                        \"comment\": reply_snippet[\"textDisplay\"],\n",
        "                        \"date\": reply_snippet[\"publishedAt\"],\n",
        "                        \"like_count\": reply_snippet[\"likeCount\"]\n",
        "                    })\n",
        "\n",
        "        page_token = response.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    comments_df = pd.DataFrame(comments).sort_values(\"date\", ascending=True)\n",
        "    comments_df = comments_df[comments_df[\"comment\"].str.len() > 20].reset_index(drop=True) # 文字数が20文字以下のテキストは除外\n",
        "\n",
        "    return comments_df\n",
        "\n",
        "api_key = \"\" # ここにAPI Keyを入力\n",
        "video_id = \"7t3lEUrCA14\"\n",
        "comments_df = get_comment(api_key, video_id)\n",
        "comments_df"
      ],
      "metadata": {
        "id": "pz5ExVSpvCMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文字列データのクレンジング\n",
        "\n",
        "収集したデータには不要な文字列が含まれている場合があるため，前処理としてテキストをクレンジングする。"
      ],
      "metadata": {
        "id": "dRXWfLeUAfTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in comments_df[\"comment\"].head(10):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "V_SCfcdUEPql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.normalize(\"NFKC\") #Unicode正規化\n",
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.replace(r\"<.*?>|[a-zA-Z0-9!@#$%^&*()_+={}\\[\\]:;<>,.?~\\\\/-]\", \"\", regex=True) #HTMLタグ，数字・記号のみの単語を除去\n",
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.strip() #先頭や末尾のスペースを削除\n",
        "comments_df"
      ],
      "metadata": {
        "id": "Rw6x_PS7zl2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 形態素解析\n",
        "\n",
        "前処理したテキストデータに対して，形態素解析を行う。Pythonで実装された形態素解析のためのツールである`Janome`を用いる。"
      ],
      "metadata": {
        "id": "eLhFOH9nAGUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = comments_df.iloc[1, 1]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "V2elzgRYALPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "tokens = t.tokenize(text)\n",
        "for token in tokens:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "HyceSsG_E8eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 特徴語の抽出\n",
        "\n",
        "全てのテキストデータに対して，形態素解析を行い，名詞のみを抽出する。"
      ],
      "metadata": {
        "id": "UjsPGCwkBc60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_janome(text):\n",
        "  tokens = []\n",
        "  for token in t.tokenize(text):\n",
        "    pos_info = token.part_of_speech.split(\",\")\n",
        "    pos = pos_info[0]\n",
        "    pos_detail1 = pos_info[1] if len(pos_info) > 1 else \"\"\n",
        "    if pos not in [\"名詞\"]:\n",
        "      continue\n",
        "    if pos == \"名詞\" and pos_detail1 in [\"代名詞\", \"数\", \"接尾\", \"非自立\", \"接続詞的\"]:\n",
        "      continue\n",
        "\n",
        "    tokens.append(token.surface)\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "comments_df[\"tokens\"] = comments_df[\"comment\"].apply(tokenize_janome)\n",
        "comments_df"
      ],
      "metadata": {
        "id": "A7DfhjDoBevT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`scikit-learn`には，テキストデータからベクトル表現を獲得するための関数がある。`CountVecorizer`関数を用いて，出現頻度を特徴量とした単語文書行列を作る。"
      ],
      "metadata": {
        "id": "XIHulADSZYaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 出現頻度の単語文書行列を作成\n",
        "vectorizer = CountVectorizer(min_df=2, max_df=0.8)  # 最低2回出現、80%以上の文書に出現する単語は除外\n",
        "word_doc_matrix = vectorizer.fit_transform(comments_df[\"tokens\"])\n",
        "\n",
        "# DataFrameに変換\n",
        "count_matrix = pd.DataFrame(\n",
        "    word_doc_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=comments_df.index\n",
        ")\n",
        "count_matrix"
      ],
      "metadata": {
        "id": "F_nwm3l-Ht0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TfidfVectorizer`関数を用いて，TF-IDFを特徴量とした単語文章行列を作る。"
      ],
      "metadata": {
        "id": "wOwIjf-XZ1eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDFの単語文書行列を作成\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.8)  # 最低2回出現、80%以上の文書に出現する単語は除外\n",
        "word_doc_matrix = vectorizer.fit_transform(comments_df[\"tokens\"])\n",
        "\n",
        "# DataFrameに変換\n",
        "tfidf_matrix = pd.DataFrame(\n",
        "    word_doc_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=comments_df.index\n",
        ")\n",
        "tfidf_matrix"
      ],
      "metadata": {
        "id": "jeaD48c3J-lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# あるテキストにおける出現頻度・Tf-IDF上位5単語を抽出\n",
        "def show_top_words(dtm, doc_idx, top_n=10):\n",
        "    row = count_matrix.loc[doc_idx]\n",
        "    top_words = (\n",
        "        row[row > 0]\n",
        "        .sort_values(ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    print(\"テキスト\", comments_df[\"comment\"][doc_idx])\n",
        "\n",
        "    print(\"\\n出現頻度が上位の単語\")\n",
        "    for word, value in top_words.items():\n",
        "        print(f\"{word}: {value}\")\n",
        "\n",
        "    row = tfidf_matrix.loc[doc_idx]\n",
        "    top_words = (\n",
        "        row[row > 0]\n",
        "        .sort_values(ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    print(\"\\nTF-IDFが上位の単語\")\n",
        "    for word, value in top_words.items():\n",
        "        print(f\"{word}: {value}\")\n",
        "\n",
        "target_index = 0 # ここの数字を変更\n",
        "show_top_words(count_matrix, doc_idx=target_index, top_n=5)"
      ],
      "metadata": {
        "id": "JwntPyqvXUX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ベクトル表現\n",
        "\n",
        "ベクトル表現間の類似度を計算することで，テキスト間の内容の類似性を定量化できる。`scikit-learn`の`cosine_similarity`関数を用いて，コサイン類似度を計算する。"
      ],
      "metadata": {
        "id": "7AM08d4mBgl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_index = 0\n",
        "text = comments_df.iloc[target_index, 1]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "klSlQneMBjr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDFによるベクトル表現\n",
        "target_vector = tfidf_matrix.iloc[target_index, :].values\n",
        "similarity = cosine_similarity(target_vector.reshape(1, -1),  tfidf_matrix)\n",
        "comments_df[\"similarity\"] = similarity[0]\n",
        "comments_df.sort_values(\"similarity\", ascending=False).iloc[1:6, :]"
      ],
      "metadata": {
        "id": "-xnIrjeQT92E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "近年はTF-IDFなどの古典的な特徴量だけではなく，BERTやGPTなどの深層学習手法をベクトル表現の獲得に用いることが多い。その例として，`SentenceTransformer`を用いて，BERTによるテキストのベクトル表現を計算し，TF-IDFによるベクトル表現と比較する。"
      ],
      "metadata": {
        "id": "xQD31zQka5CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embeddings = model.encode(comments_df[\"comment\"].to_list(), normalize_embeddings=True)\n",
        "target_vector = embeddings[target_index]\n",
        "similarity = cosine_similarity(target_vector.reshape(1, -1),  embeddings)\n",
        "comments_df[\"similarity\"] = similarity[0]\n",
        "comments_df.sort_values(\"similarity\", ascending=False).iloc[1:6, :]"
      ],
      "metadata": {
        "id": "mJ2_tpaKUtzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストデータのベクトル表現をUMAPで2次元削減した上で可視化\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "reducer = umap.UMAP(n_components=2, random_state=42, n_jobs=1)\n",
        "tfidf_2d = reducer.fit_transform(tfidf_matrix.values)\n",
        "axes[0].scatter(\n",
        "    tfidf_2d[:, 0],\n",
        "    tfidf_2d[:, 1],\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "axes[0].set_title('TF-IDF')\n",
        "axes[0].set_xticks([])\n",
        "axes[0].set_yticks([])\n",
        "\n",
        "embeddings_2d = reducer.fit_transform(embeddings)\n",
        "axes[1].scatter(\n",
        "    embeddings_2d[:, 0],\n",
        "    embeddings_2d[:, 1],\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "axes[1].set_title('BERT')\n",
        "axes[1].set_xticks([])\n",
        "axes[1].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0xhkd9LFY8J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## テキスト分類\n",
        "\n",
        "BERTによるベクトル表現を用いて，テキストデータから類似したテキスト集合をクラスターとして分類する。分類には，k-means法（`scikit-learn`の`kMeans`関数）を用いる。"
      ],
      "metadata": {
        "id": "1aWts9GspJ8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 2 # ここにクラスター数を入力\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# DataFrameに付与\n",
        "comments_df = comments_df.copy()\n",
        "comments_df[\"cluster\"] = labels\n",
        "comments_df[\"embeddings_1\"] = embeddings_2d[:, 0]\n",
        "comments_df[\"embeddings_2\"] = embeddings_2d[:, 1]"
      ],
      "metadata": {
        "id": "fYmy0lyEpUo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# クラスターごとに色分けして可視化\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    x=\"embeddings_1\",\n",
        "    y=\"embeddings_2\",\n",
        "    hue=\"cluster\",\n",
        "    data=comments_df,\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UWzJm8ntp0lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれのクラスターのテキスト集合から各単語のTF-IDFの平均値を計算し，上位の単語を示す。"
      ],
      "metadata": {
        "id": "89ztVYr2cfZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = sorted(comments_df[\"cluster\"].unique())\n",
        "n_clusters = len(clusters)\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(n_clusters / n_cols)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_rows, n_cols,\n",
        "    figsize=(4 * n_cols, 3 * n_rows),\n",
        "    sharex=False,\n",
        "    sharey=False\n",
        ")\n",
        "\n",
        "# axes を常に 1 次元で扱えるようにする\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, c in zip(axes, clusters):\n",
        "    idx = comments_df[\"cluster\"] == c\n",
        "\n",
        "    word_freq = tfidf_matrix.loc[idx].mean(axis=0).sort_values(ascending=False)\n",
        "    top10_words = word_freq.head(10)\n",
        "\n",
        "    ax.barh(range(len(top10_words)), top10_words.values)\n",
        "    ax.set_yticks(range(len(top10_words)))\n",
        "    ax.set_yticklabels(top10_words.index)\n",
        "    ax.set_xlabel(\"平均TF-IDF\")\n",
        "    ax.set_title(f\"Cluster {c}\")\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "# 余った subplot を消す\n",
        "for ax in axes[n_clusters:]:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w5uklRLIrxGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 演習：任意のYoutubeの動画コメントのテキスト分析\n",
        "\n",
        "演習1：動画IDを指定し，Youtube動画のコメントを収集する。"
      ],
      "metadata": {
        "id": "yLTWgYMLBvs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = \"\" # ここにVideo IDを入力\n",
        "comments_df = get_comment(api_key, video_id)\n",
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.normalize(\"NFKC\") #Unicode正規化\n",
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.replace(r\"<.*?>|[a-zA-Z0-9!@#$%^&*()_+={}\\[\\]:;<>,.?~\\\\/-]\", \"\", regex=True) #HTMLタグ，数字・記号のみの単語を除去\n",
        "comments_df[\"comment\"] = comments_df[\"comment\"].str.strip() #先頭や末尾のスペースを削除\n",
        "comments_df[\"tokens\"] = comments_df[\"comment\"].apply(tokenize_janome)\n",
        "comments_df"
      ],
      "metadata": {
        "id": "8yDqqM8DBxQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDFの単語文書行列を作成\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.8)  # 最低2回出現、80%以上の文書に出現する単語は除外\n",
        "word_doc_matrix = vectorizer.fit_transform(comments_df[\"tokens\"])\n",
        "\n",
        "# DataFrameに変換\n",
        "tfidf_matrix = pd.DataFrame(\n",
        "    word_doc_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=comments_df.index\n",
        ")\n",
        "tfidf_matrix"
      ],
      "metadata": {
        "id": "UyIhLk93bKWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTによるベクトル表現の獲得\n",
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "embeddings = model.encode(comments_df[\"comment\"].to_list(), normalize_embeddings=True)\n",
        "reducer = umap.UMAP(n_components=2, random_state=42, n_jobs=1)\n",
        "embeddings_2d = reducer.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "1ev1RiDIdBN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "演習2：テキスト分類を行い，それぞれのクラスターの特徴語を抽出する。クラスター数を変化させて，コメントデータにどのようなトピックが含まれているかを調べる。"
      ],
      "metadata": {
        "id": "TdJxsPE5dm1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters =  # ここにクラスター数を入力\n",
        "kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# DataFrameに付与\n",
        "comments_df = comments_df.copy()\n",
        "comments_df[\"cluster\"] = labels\n",
        "comments_df[\"embeddings_1\"] = embeddings_2d[:, 0]\n",
        "comments_df[\"embeddings_2\"] = embeddings_2d[:, 1]\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.scatterplot(\n",
        "    x=\"embeddings_1\",\n",
        "    y=\"embeddings_2\",\n",
        "    hue=\"cluster\",\n",
        "    data=comments_df,\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "\n",
        "plt.title(\"BERT\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BxNv37xftccF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = sorted(comments_df[\"cluster\"].unique())\n",
        "n_clusters = len(clusters)\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = math.ceil(n_clusters / n_cols)\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_rows, n_cols,\n",
        "    figsize=(4 * n_cols, 3 * n_rows),\n",
        "    sharex=False,\n",
        "    sharey=False\n",
        ")\n",
        "\n",
        "# axes を常に 1 次元で扱えるようにする\n",
        "axes = axes.flatten()\n",
        "\n",
        "for ax, c in zip(axes, clusters):\n",
        "    idx = comments_df[\"cluster\"] == c\n",
        "\n",
        "    word_freq = tfidf_matrix.loc[idx].mean(axis=0).sort_values(ascending=False)\n",
        "    top10_words = word_freq.head(10)\n",
        "\n",
        "    ax.barh(range(len(top10_words)), top10_words.values)\n",
        "    ax.set_yticks(range(len(top10_words)))\n",
        "    ax.set_yticklabels(top10_words.index)\n",
        "    ax.set_xlabel(\"平均TF-IDF\")\n",
        "    ax.set_title(f\"Cluster {c}\")\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "# 余った subplot を消す\n",
        "for ax in axes[n_clusters:]:\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "81HT6dwyteUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9K0qgMSh5Rx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}